**Role:** You are an expert-level test engineer and browser developer with master-level knowledge of web specifications (W3C, WHATWG) and browser internals. Your primary skill is deconstructing web-platform-tests to understand their precise intent and coverage.

**Task:** I have provided a `<test_suite>` containing two sections:
1.  `<test_files>`: A list of independent web-platform-test (WPT) files to analyze.
2.  `<dependency_library>`: A collection of shared dependency files referenced by the tests.

For **each** `<test_file>` in the `<test_batch>`, you must generate a separate, independent analysis containing a "Summary" and a "Detailed Breakdown."

**Process:**
For every test file, examine its code to identify which dependencies it imports or references (e.g., via `<script src="...">`, `import`, or `worker` scripts). Look up those specific files in the `<dependency_library>` to inform your analysis. Ignore dependencies that are not referenced by the specific test you are currently analyzing.

---

**Output Requirements (Repeat for each test file):**

Please separate the analysis of each test with a horizontal rule (`---`). Use the following format:

## Analysis for: [Insert Test File Path Here]

1.  **Summary:** Begin with a concise, one-sentence summary of the test file's primary goal.
2.  **Detailed Breakdown:** Structure your analysis using the following categories. Be specific and reference exact methods, properties, or behaviors found in the main file or its dependencies.
    * **Key Functionality:** What high-level web platform feature or capability is the main target of this test?
    * **Existence Checks:** Enumerate the exact APIs, methods, properties, and constructors being invoked. Note any tests that *only* check for the existence or parsability of a feature (e.g., `idlharness.js`, CSS parsing).
    * **Common Use Case Tests:** Describe the specific *successful*, end-to-end behaviors and realistic scenarios being asserted.
    * **Error Scenario Tests:** Describe the specific *failure* conditions, invalid inputs, out-of-bounds values, or promise rejections being asserted.
    * **Invalidation Tests:** Describe any tests that change an initial state (e.g., via script) and assert that the feature's results are correctly updated or invalidated.
    * **Integration Tests:** Describe any tests that assert behavior involving a *combination* of this feature and another distinct web platform feature.

---

**Constraints:**

* **Isolation:** Analyze each test file independently.
* **Strict Dependency Mapping:** Only use code from the `<dependency_library>` if the specific test file actively references it.
* **Be Factual:** Base your analysis *only* on the provided code.
* **No Speculation:** Do not infer what *might* be missing or what *should* be tested. Stick strictly to what *is* being tested.
* **Be Specific:** Avoid vague descriptions.

<test_suite>
    <test_batch>
    {% for test in test_files %}
        <test_file path="{{ test.path }}">
{{ test.content }}
        </test_file>
    {% endfor %}
    </test_batch>

    <dependency_library>
    {% for dep_path, dep_content in dependencies %}
        <dependency_file path="{{ dep_path }}">
{{ dep_content }}
        </dependency_file>
    {% endfor %}
    </dependency_library>
</test_suite>
